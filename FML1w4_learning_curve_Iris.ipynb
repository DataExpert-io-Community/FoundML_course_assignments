{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating the quality of classifiers\n",
    "## The learning curve and the decision regions of several classifiers on the Iris dataset\n",
    "### This notebook is based on the notebook by Gabriel Shiu available on Kaggle at https://www.kaggle.com/code/gabrielshiu/iris-investigation-voting-learning-curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from sklearn.inspection import plot_partial_dependence\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the seed of the random number generator, for reproducibility purposes\n",
    "\n",
    "import os\n",
    "\n",
    "def reset_seed(SEED = 0):\n",
    "    \"\"\"Reset the seed for every random library in use (System, numpy)\"\"\"\n",
    "\n",
    "    os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "\n",
    "reset_seed(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Iris dataset from the sklearn library. \n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X, y = load_iris(return_X_y=True, as_frame=True)\n",
    "\n",
    "# We already know from an earlier assignment that the sepal features do not help much with the classification\n",
    "# We drop them. \n",
    "X.drop(['sepal length (cm)', 'sepal width (cm)'], axis=1, inplace=True)\n",
    "\n",
    "# Split into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X.values, \n",
    "    y, \n",
    "    test_size=0.20, \n",
    "    shuffle=True,\n",
    "    random_state=100,\n",
    "    stratify=y,\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "stand_scaler = StandardScaler()\n",
    "stand_scaler = stand_scaler.fit(X_train)\n",
    "X_train = stand_scaler.transform(X_train)\n",
    "X_test = stand_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# We evaluate several different classifiers on the Iris dataset\n",
    "# To reduce the chance that the performance somehow comes from the specific validation dataset, \n",
    "#     we do a 10-fold cross-validation.\n",
    "# The data is split into 10 equal bins (in a stratified way). 9 of them are used to train, 1 to validate.\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "\n",
    "# Here is the list of models we will evaluate, most already used in previous assignments\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "estimators = [\n",
    "    KNeighborsClassifier(n_neighbors=5),\n",
    "    SVC(),\n",
    "    DecisionTreeClassifier(max_depth=None),\n",
    "    RandomForestClassifier(n_estimators=100, max_depth=None),\n",
    "    GaussianNB(),\n",
    "    LogisticRegression(max_iter=1000),\n",
    "    BernoulliNB(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 18\n",
    "BIGGER_SIZE = 22\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = pd.DataFrame(columns=['Estimator', 'CV Scores mean', 'CV Scores Variance'])\n",
    "\n",
    "for i in range(len(estimators)):\n",
    "    est = estimators[i]\n",
    "    est_name = est.__class__.__name__\n",
    "    est.fit(X_train, y_train)\n",
    "    cv_scores = cross_val_score(est, X_train, y_train, cv=skf, n_jobs=-1)\n",
    "    scores.loc[i] = [est_name, cv_scores.mean(), cv_scores.std()**2]\n",
    "    \n",
    "scores.sort_values(by='CV Scores mean', ascending=False, inplace=True)\n",
    "print(scores)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.barplot(x=scores['CV Scores mean'], y=scores['Estimator'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curves\n",
    "#### Source: https://towardsdatascience.com/learning-curve-to-identify-overfitting-underfitting-problems-133177f38df5\n",
    "\n",
    "Learning curves plot the training and validation loss of a sample of training examples by incrementally adding new training examples. Learning curves help us in identifying whether adding additional training examples would improve the validation score (score on unseen data). If a model is overfit, then adding additional training examples might improve the model performance on unseen data. Similarly, if a model is underfit, then adding training examples doesn’t help. \n",
    "\n",
    "#### Typical features of the learning curve of a well fit model\n",
    "- Training loss and Validation loss are close to each other with validation loss being slightly greater than the training loss.\n",
    "- Initially decreasing training and validation loss and a pretty flat training and validation loss after some point till the end.\n",
    "\n",
    "#### Typical features of the learning curve of an overfit model\n",
    "- Training loss and Validation loss are far away from each other.\n",
    "- Gradually decreasing validation loss (without flattening) upon adding training examples.\n",
    "- Very low training loss that’s very slightly increasing upon adding training examples.\n",
    "\n",
    "#### Typical features of the learning curve of an underfit model\n",
    "- Increasing training loss upon adding training examples.\n",
    "- Training loss and validation loss are close to each other at the end.\n",
    "- Sudden dip in the training loss and validation loss at the end (not always)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how the model learns with more data\n",
    "# Train with just 10% of the training data, then with 20%, etc.\n",
    "# We should see how the model quality evolves with more data to train on\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(model, X, y, title, cv, train_sizes, plot_location):\n",
    "    train_sizes, train_scores, valid_scores = learning_curve(\n",
    "        model, X, y, cv=cv, n_jobs=-1, random_state=0, train_sizes=train_sizes\n",
    "    )\n",
    "    \n",
    "    ax = plt.subplot(4,2,plot_location)    \n",
    "    ax.set(title=title)\n",
    "    ylim = (0.4, 1.01)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    valid_scores_mean = np.mean(valid_scores, axis=1)\n",
    "    valid_scores_std = np.std(valid_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                train_scores_mean + train_scores_std, alpha=0.3,\n",
    "                color=\"g\")\n",
    "    plt.fill_between(train_sizes, valid_scores_mean - valid_scores_std,\n",
    "                valid_scores_mean + valid_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"g\",\n",
    "        label=\"Mean train score\")\n",
    "    plt.plot(train_sizes, valid_scores_mean, 'o-', color=\"r\",\n",
    "        label=\"Mean CV score\")\n",
    "    plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves on the training and on the validation datasets\n",
    "\n",
    "plt.figure(figsize=(20,35))\n",
    "\n",
    "for i in range(len(estimators)):\n",
    "    plot_learning_curve(estimators[i], X_train, y_train,\n",
    "                        estimators[i].__class__.__name__, skf, np.linspace(0.1, 0.9, 20),\n",
    "                        plot_location=i+1\n",
    "                       )    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some conclusions about these models\n",
    "- Bernoulli naive Bayes has the lowest score. The learning curves on the training and on the validation data are close to each other with enough training data. This shows the model is well fit. The reason why it does not have a good score is probably because he data is simply not Bernoulli distributed.\n",
    "- The other 7 models have excellent cross-validation scores (calculated as the average of all CV models' score on their own validation data sets). Some differences between them can be seen in the learning curves. \n",
    "- The decision tree and the random forest classifiers have the biggest gap between the learning curves on the training and on the validation data sets. Also, the band on the training set is very narrow, showing its ability to fit well no matter of the change in the training set. Worse, the training learning curve is about one standard deviation away from the validation learning curve. This suggests that these models are over-fitted. \n",
    "- The other models seem to be well fit: the two learning curves train/validation are close to each other, their +/- standard deviation bands overlap, with more variation of course on the validation band. Their score stabilises quickly, suggesting they can fit well even with relatively little training data from the Iris dataset. \n",
    "- The models differ widely in HOW they make their predictions. We can visualise this below through plotting the decision boundaries. See if you can confirm the points made here after seeing the decision boundaries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision boundaries\n",
    "\n",
    "A decision boundary is a set of hyperplanes that split the n-dimensional (in our case 2-dim) space of the data points into th K classes that the model was trained for. The model is fit using the training dataset and then it is ready to make predictions about the class of any datapoint in the n-dimensional space. The decision boundaries are the hyperplanes separating the sub-regions corresponding to each of the K classes. The shape of these decision boundaries are different from a model to another, depending on the mathematical functions underlying each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# Plotting decision regions\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(20,35))\n",
    "\n",
    "for i in range(len(estimators)):\n",
    "    axes[i//2, i%2].set(title=estimators[i].__class__.__name__)\n",
    "    disp = DecisionBoundaryDisplay.from_estimator(\n",
    "        estimators[i], X_train, response_method=\"predict\",\n",
    "        xlabel=X.columns[0], ylabel=X.columns[1],\n",
    "        alpha=0.5,\n",
    "        ax=axes[i//2, i%2],\n",
    "        plot_method='contourf'\n",
    "    )\n",
    "    disp.ax_.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolor=\"k\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4\n",
    "\n",
    "- Load the Wisconsin breast cancer dataset from OpenML (ID 43757) in the same way you did in assignment 3. \n",
    "- Train the same 8 classifiers we used in the tutorial part of this notebook, display their CV scores, learning curves and decision boundaries as done above. \n",
    "- Which models seem overfit?\n",
    "- Focus on the decision tree and on the random forest models. Train them in 4 different variants, with the parameter min_samples_split taking values 2, 5, 10, and 100. This is the parameter controlling the minimum number of datapoints in an internal node (an internal node is one that the model decides to improve through a further query; any node that gets less points than min_samples_split becomes a leaf in the tree/forest). Looking at their learning curves (their scores are close to each other), which one seems the best fit model?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
