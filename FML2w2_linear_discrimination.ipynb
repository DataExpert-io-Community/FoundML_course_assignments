{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "#### Part of the course on \"Foundations of machine learning\", Department of Mathematics and Statistics, University of Turku, Finland\n",
    "#### Lectures available on YouTube: https://youtube.com/playlist?list=PLbkSohdmxoVAZ9DEHEWHjeGK7Ei-DjKHI&si=Msu74_I0qhLrRWcu\n",
    "#### Code available on GitHub: https://github.com/ionpetre/FoundML_course_assignments\n",
    "\n",
    "#### This notebook is based on the following sources: \n",
    "\n",
    "> https://www.kaggle.com/code/prashant111/logistic-regression-classifier-tutorial/\n",
    "\n",
    "Linear discrimination is a classification approach where the objective is to learn a simple, linear function separating our classes. Rather than learning the distribution of the classes, this approach focuses on learning the separation of the classes, a problem that may often be simpler to solve. We discuss about pairwise separation, about gradient descent, and about logistic discrimination. \n",
    "\n",
    "In this noteebook we use the UCI heart disease dataset for the tutorial part and the NNN datasset for the challenge part. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-09T17:34:11.060373Z",
     "iopub.status.busy": "2021-09-09T17:34:11.059927Z",
     "iopub.status.idle": "2021-09-09T17:34:12.616363Z",
     "shell.execute_reply": "2021-09-09T17:34:12.615487Z",
     "shell.execute_reply.started": "2021-09-09T17:34:11.06034Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix, classification_report\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the seed of the random number generator, for reproducibility purposes\n",
    "\n",
    "import os\n",
    "\n",
    "def reset_seed(SEED = 0):\n",
    "    \"\"\"Reset the seed for every random library in use (System, numpy)\"\"\"\n",
    "\n",
    "    os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "\n",
    "reset_seed(220)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Demo decision trees on the UCI heart disease dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The UCI heart disease dataset: https://archive.ics.uci.edu/dataset/45/heart+disease\n",
    "\n",
    "The UCI Heart Disease dataset is a well-known dataset used in machine learning and data analysis to study and predict the presence of heart disease in individuals. It is often referred to as the \"Cleveland Heart Disease dataset\" because it was collected from the Cleveland Clinic Foundation in the late 1980s.\n",
    "\n",
    "Here are some key details about the UCI Heart Disease dataset:\n",
    "\n",
    "1. Data Source: The dataset was collected from the Cleveland Clinic Foundation's Heart Disease Institute. The original dataset had several contributors, including Robert Detrano, Don Brownlee, and Wesley Turner.\n",
    "\n",
    "2. Data Description: The UCI Heart Disease dataset consists of 303 instances, each representing a patient, and contains 76 attributes. However, only 14 of these attributes are typically used in analysis and modeling. These attributes include features such as age, sex, chest pain type, resting blood pressure, cholesterol level, maximum heart rate, exercise-induced angina, and more.\n",
    "\n",
    "3. Target Variable: The primary target variable in this dataset is the presence of heart disease, where '0' typically indicates no heart disease and '1' indicates the presence of heart disease. This binary classification task makes it a popular choice for predictive modeling.\n",
    "\n",
    "4. Purpose: The UCI Heart Disease dataset is commonly used for research, practice, and educational purposes in the field of cardiovascular medicine, as well as in machine learning and data science. Researchers and data scientists use this dataset to develop predictive models for diagnosing heart disease and assessing cardiovascular risk factors.\n",
    "\n",
    "5. Data Availability: The UCI Heart Disease dataset is publicly available and can be accessed through the UCI Machine Learning Repository or various data science platforms and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "X, _ = fetch_openml(\n",
    "    data_id=43823,\n",
    "    as_frame=True,\n",
    "    return_X_y=True,\n",
    "    parser = 'auto'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The target feature is 'Heart_Disease'. We save it in the variable y and encode it as 0/1.\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = pd.DataFrame(le.fit_transform(X['Heart_Disease']))\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop the target feature 'Heart_Disease' from the X dataset. \n",
    "\n",
    "X = X.drop('Heart_Disease', axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_valid, X_test, y_train_valid, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.2, \n",
    "    random_state=150, \n",
    "    stratify=y,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_valid, \n",
    "    y_train_valid, \n",
    "    test_size=0.25, \n",
    "    random_state=150, \n",
    "    stratify=y_train_valid,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# convert to pandas dataframe\n",
    "X_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "X_valid = pd.DataFrame(X_valid, columns=X.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X.columns)\n",
    "y_train = pd.DataFrame(y_train, columns=y.columns)\n",
    "y_valid = pd.DataFrame(y_valid, columns=y.columns)\n",
    "y_test = pd.DataFrame(y_test, columns=y.columns)\n",
    "\n",
    "del X\n",
    "del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise the data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "stand_scaler = StandardScaler()\n",
    "stand_scaler.fit(X_train)\n",
    "\n",
    "X_train_std = pd.DataFrame(stand_scaler.transform(X_train), columns=X_train.columns)\n",
    "X_valid_std = pd.DataFrame(stand_scaler.transform(X_valid), columns=X_valid.columns)\n",
    "X_test_std  = pd.DataFrame(stand_scaler.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_std.info(), \"\\n\", X_valid_std.info(), \"\\n\", X_test_std.info())\n",
    "print(y_train.value_counts(), \"\\n\", y_valid.value_counts(), \"\\n\", y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a logistic regrerssor on the standardised data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_clf = LogisticRegression(\n",
    "    penalty='l2', \n",
    "    multi_class='auto', \n",
    "    max_iter=100, \n",
    "    random_state=10,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "log_clf.fit(X_train_std, np.ravel(y_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the quality of the predictions through the accuracy score (on train and validation) and through the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classes = ['NO','YES']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def plot_confusionmatrix(y_train_pred, y_train, dom):\n",
    "    print(f'{dom} confusion matrix:')\n",
    "    cf = confusion_matrix(y_train_pred,y_train, normalize=None)\n",
    "    plt.figure(figsize=(4,3))\n",
    "    sns.heatmap(cf, annot=True, yticklabels=classes, xticklabels=classes, cmap='Blues', fmt='g')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = log_clf.predict(X_train_std)\n",
    "y_valid_pred = log_clf.predict(X_valid_std)\n",
    "\n",
    "print(\"The classification results on the training data:\")\n",
    "print(classification_report(y_train,y_train_pred))\n",
    "print(\"Confusion matrix (training data):\\n\", confusion_matrix(y_train,y_train_pred))\n",
    "\n",
    "print(\"\\n The classification results on the validation data:\")\n",
    "print(classification_report(y_valid,y_valid_pred))\n",
    "print(\"Confusion matrix (validation data):\\n\", confusion_matrix(y_valid,y_valid_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with fewer features\n",
    "\n",
    "We can use a decision tree as a feature ranking tool, and train the logisitc regression model on the top ranked features. We train the decision tree, and the extract the top features from the trained model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a decision tree classifier with its default setup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = tree.DecisionTreeClassifier(\n",
    "    ccp_alpha = 0,\n",
    "    class_weight = None,\n",
    "    criterion = 'gini',\n",
    "    max_depth = 3,\n",
    "    max_features = None,\n",
    "    max_leaf_nodes = 10,\n",
    "    min_impurity_decrease = 0.0,\n",
    "    min_samples_leaf = 1,\n",
    "    min_samples_split = 2,\n",
    "    min_weight_fraction_leaf = 0.0,\n",
    "    random_state = 2023,\n",
    "    splitter = 'best',\n",
    ")\n",
    "\n",
    "tree_clf.fit(X_train_std,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the decision tree. \n",
    "Each node is shown with its internal characteristics: the decision rule, the impurity, the class balance. The decision that could be made in each node based on the class balance is indicated through the coloring of the node. The more orange the node, the clearer the decision for \"Not heart disease\". The more blue the node, the clearer the decision for \"Heart disease\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(60,30))\n",
    "features = list(X_train.columns.values)\n",
    "classes = ['NO','YES']\n",
    "tree.plot_tree(\n",
    "    tree_clf,\n",
    "    feature_names = features,\n",
    "    class_names = classes,\n",
    "    filled = True,\n",
    "    proportion = False,\n",
    "    impurity = True, \n",
    "    rounded = True,\n",
    "    fontsize = 16\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model's performance on the test dataset\n",
    "\n",
    "y_train_pred = tree_clf.predict(X_train_std)\n",
    "y_valid_pred = tree_clf.predict(X_valid_std)\n",
    "\n",
    "\n",
    "print(\"The classification results on the training data:\")\n",
    "print(f'Training score {accuracy_score(y_train, y_train_pred)}')\n",
    "print(classification_report(y_train,y_train_pred))\n",
    "print(\"Confusion matrix (training data):\\n\", confusion_matrix(y_train,y_train_pred))\n",
    "\n",
    "print(\"\\n The classification results on the validation data:\")\n",
    "print(f'Validation score {accuracy_score(y_valid, y_valid_pred)}')\n",
    "print(classification_report(y_valid,y_valid_pred))\n",
    "print(\"Confusion matrix (validation data):\\n\", confusion_matrix(y_valid,y_valid_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the feature ranking from the decision tree, sort it by the ranking score\n",
    "\n",
    "feat_imp = pd.DataFrame(tree_clf.feature_importances_, index=X_train_std.columns, columns =[\"Score\"])\n",
    "feat_imp.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a new logistic regression model, just on the features with non-zero ranking score in the decision tree model\n",
    "\n",
    "log_clf_v2 = LogisticRegression(\n",
    "    penalty='l2', \n",
    "    multi_class='auto', \n",
    "    max_iter=100, \n",
    "    random_state=10,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "features = [\"Chest_pain_type\", \n",
    "            \"ST_depression\", \n",
    "            \"Slope_of_ST\", \n",
    "            \"Number_of_vessels_fluro\",\n",
    "            \"Max_HR\",\n",
    "            \"BP\"\n",
    "           ]\n",
    "\n",
    "log_clf_v2.fit(X_train_std[features], np.ravel(y_train))\n",
    "\n",
    "y_train_pred = log_clf_v2.predict(X_train_std[features])\n",
    "y_valid_pred = log_clf_v2.predict(X_valid_std[features])\n",
    "\n",
    "print(\"\\n The classification results on the training data:\")\n",
    "print(f'Train score {accuracy_score(y_train, y_train_pred)}')\n",
    "print(classification_report(y_train,y_train_pred))\n",
    "print(\"Confusion matrix (training data):\\n\", confusion_matrix(y_train,y_train_pred))\n",
    "\n",
    "print(\"\\n The classification results on the validation data:\")\n",
    "print(f'Validation score {accuracy_score(y_valid, y_valid_pred)}')\n",
    "print(classification_report(y_valid,y_valid_pred))\n",
    "print(\"Confusion matrix (validation data):\\n\", confusion_matrix(y_valid,y_valid_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "We conclude that the logistic regression model has about the same performance on the full feature space as on the smaller features. Since the number of features is relatively small, we may continue using the original features. \n",
    "\n",
    "#### We check the performance of the model on the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = log_clf.predict(X_test_std)\n",
    "\n",
    "print(\"\\n The classification results on the test data:\")\n",
    "print(f'Test score {accuracy_score(y_test, y_test_pred)}')\n",
    "print(classification_report(y_test,y_test_pred))\n",
    "print(\"Confusion matrix (test data):\\n\", confusion_matrix(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision boundary visualisation\n",
    "To visualise the decision boundaries we have to transform the data in 2D and we do this through PCA. We also retrain the models on the transformed data. This is only for visualisation purposes, the classifications should be done in the original data space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA transformation into 2D\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_model = PCA(n_components = 2)\n",
    "pca_model.fit(X_train_std, y_train.values.ravel())\n",
    "\n",
    "X_train_std_pca = pca_model.transform(X_train_std)\n",
    "X_valid_std_pca = pca_model.transform(X_valid_std)\n",
    "X_test_std_pca = pca_model.transform(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new models for the visualisation in 2D: logistic regression and decision tree\n",
    "\n",
    "log_clf_2D = LogisticRegression(\n",
    "    penalty='l2', \n",
    "    multi_class='auto', \n",
    "    max_iter=100, \n",
    "    random_state=10,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "tree_clf_2D = tree.DecisionTreeClassifier(\n",
    "    ccp_alpha = 0,\n",
    "    class_weight = None,\n",
    "    criterion = 'gini',\n",
    "    max_depth = 3,\n",
    "    max_features = None,\n",
    "    max_leaf_nodes = 10,\n",
    "    min_impurity_decrease = 0.0,\n",
    "    min_samples_leaf = 1,\n",
    "    min_samples_split = 2,\n",
    "    min_weight_fraction_leaf = 0.0,\n",
    "    random_state = 2023,\n",
    "    splitter = 'best',\n",
    ")\n",
    "\n",
    "classifiers = [log_clf_2D, tree_clf_2D]\n",
    "\n",
    "# Train the models on the PCA-transformed data, just for visualisation purposes\n",
    "\n",
    "for i in range(len(classifiers)):    \n",
    "    classifiers[i].fit(X_train_std_pca, np.ravel(y_train))\n",
    "\n",
    "\n",
    "\n",
    "from itertools import product\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# Plot the decision regions on the training, on the validation, and on the test data\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8,4))\n",
    "\n",
    "for i in range(len(classifiers)):\n",
    "    axes[i].set(title=classifiers[i].__class__.__name__)\n",
    "    disp = DecisionBoundaryDisplay.from_estimator(\n",
    "        classifiers[i], X_train_std_pca, response_method=\"predict\",\n",
    "        #xlabel=X_train_std.columns, ylabel=y_train.columns,\n",
    "        alpha=0.5,\n",
    "        ax=axes[i],\n",
    "        plot_method='contourf'\n",
    "    )\n",
    "    disp.ax_.scatter(X_train_std_pca[:, 0], X_train_std_pca[:, 1], c=y_train, edgecolor=\"k\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8,4))\n",
    "\n",
    "for i in range(len(classifiers)):\n",
    "    axes[i].set(title=classifiers[i].__class__.__name__)\n",
    "    disp = DecisionBoundaryDisplay.from_estimator(\n",
    "        classifiers[i], X_valid_std_pca, response_method=\"predict\",\n",
    "        #xlabel=X_train_std.columns, ylabel=y_train.columns,\n",
    "        alpha=0.5,\n",
    "        ax=axes[i],\n",
    "        plot_method='contourf'\n",
    "    )\n",
    "    disp.ax_.scatter(X_valid_std_pca[:, 0], X_valid_std_pca[:, 1], c=y_valid, edgecolor=\"k\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8,4))\n",
    "\n",
    "for i in range(len(classifiers)):\n",
    "    axes[i].set(title=classifiers[i].__class__.__name__)\n",
    "    disp = DecisionBoundaryDisplay.from_estimator(\n",
    "        classifiers[i], X_test_std_pca, response_method=\"predict\",\n",
    "        #xlabel=X_train_std.columns, ylabel=y_train.columns,\n",
    "        alpha=0.5,\n",
    "        ax=axes[i],\n",
    "        plot_method='contourf'\n",
    "    )\n",
    "    disp.ax_.scatter(X_test_std_pca[:, 0], X_test_std_pca[:, 1], c=y_test, edgecolor=\"k\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "RocCurveDisplay.from_estimator(log_clf, X_test_std, y_test)\n",
    "plt.plot([0,1], [0,1], 'k--' )\n",
    "plt.title('ROC curve for the logistic regression classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_valid\n",
    "del y_train_valid\n",
    "del X_train\n",
    "del X_train_std\n",
    "del X_train_std_pca\n",
    "del y_train\n",
    "del X_valid\n",
    "del X_valid_std\n",
    "del X_valid_std_pca\n",
    "del y_valid\n",
    "del X_test\n",
    "del X_test_std\n",
    "del X_test_std_pca\n",
    "del y_test\n",
    "\n",
    "del log_clf\n",
    "del log_clf_v2\n",
    "del log_clf_2D\n",
    "del tree_clf\n",
    "del tree_clf_2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Rain prediction model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a logistic regression model to predict whether it will rain tomorrow. The dataset contains about 10 years of daily weather observations from many locations across Australia.\n",
    "\n",
    "Data source: http://www.bom.gov.au/climate/dwo/ and http://www.bom.gov.au/climate/data.\n",
    "Copyright Commonwealth of Australia 2010, Bureau of Meteorology.\n",
    "Data downloaded from: https://www.kaggle.com/datasets/arunavakrchakraborty/australia-weather-data\n",
    "Great data exploration analysis on this dataset: https://www.kaggle.com/code/prashant111/logistic-regression-classifier-tutorial/\n",
    "\n",
    "**Data Description**\n",
    "\n",
    "Location - Name of the city from Australia.\n",
    "MinTemp - The Minimum temperature during a particular day. (degree Celsius)\n",
    "MaxTemp - The maximum temperature during a particular day. (degree Celsius)\n",
    "Rainfall - Rainfall during a particular day. (millimeters)\n",
    "Evaporation - Evaporation during a particular day. (millimeters)\n",
    "Sunshine - Bright sunshine during a particular day. (hours)\n",
    "WindGusDir - The direction of the strongest gust during a particular day. (16 compass points)\n",
    "WindGuSpeed - Speed of strongest gust during a particular day. (kilometers per hour)\n",
    "WindDir9am - The direction of the wind for 10 min prior to 9 am. (compass points)\n",
    "WindDir3pm - The direction of the wind for 10 min prior to 3 pm. (compass points)\n",
    "WindSpeed9am - Speed of the wind for 10 min prior to 9 am. (kilometers per hour)\n",
    "WindSpeed3pm - Speed of the wind for 10 min prior to 3 pm. (kilometers per hour)\n",
    "Humidity9am - The humidity of the wind at 9 am. (percent)\n",
    "Humidity3pm - The humidity of the wind at 3 pm. (percent)\n",
    "Pressure9am - Atmospheric pressure at 9 am. (hectopascals)\n",
    "Pressure3pm - Atmospheric pressure at 3 pm. (hectopascals)\n",
    "Cloud9am - Cloud-obscured portions of the sky at 9 am. (eighths)\n",
    "Cloud3pm - Cloud-obscured portions of the sky at 3 pm. (eighths)\n",
    "Temp9am - The temperature at 9 am. (degree Celsius)\n",
    "Temp3pm - The temperature at 3 pm. (degree Celsius)\n",
    "RainToday - If today is rainy then ‘Yes’. If today is not rainy then ‘No’.\n",
    "RainTomorrow - If tomorrow is rainy then 1 (Yes). If tomorrow is not rainy then 0 (No)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset\n",
    "For this challenge, you need to download the training and the test datasets from Moodle (or from the Kaggle source above) and make sure it is saved in the same folder as your code or indicate the relative folder location in the read function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"AUS_weather_training_data.csv\")\n",
    "X_test = pd.read_csv(\"AUS_weather_test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1. How many features you have in the training dataset (not counting the target feature 'RainTomorrow')? \n",
    "#### Q2. How many data points do you have in the training set? \n",
    "#### Q3. How many data points do you have in the test set? \n",
    "#### Q4. Do you have missing values in the test set? \n",
    "#### Q5. Do you have the 'RainTomorrow' feature in the test dataset? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'row ID' feature from both sets\n",
    "\n",
    "X_train = X_train.drop(\"row ID\", axis=1)\n",
    "X_test = X_test.drop(\"row ID\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the categorical variables\n",
    "\n",
    "categorical = [var for var in X_train.columns if X_train[var].dtype=='O']\n",
    "print('There are {} categorical variables\\n'.format(len(categorical)))\n",
    "print('The categorical variables are :', categorical)\n",
    "\n",
    "# check for missing values in the categorical variables \n",
    "\n",
    "X_train[categorical].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the numerical variables\n",
    "\n",
    "numerical = [var for var in X_train.columns if X_train[var].dtype!='O']\n",
    "print('There are {} numerical variables\\n'.format(len(numerical)))\n",
    "print('The numerical variables are :', numerical)\n",
    "\n",
    "# check missing values in the numerical variables\n",
    "X_train[numerical].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the target variable from the training dataset\n",
    "\n",
    "y = X_train['RainTomorrow']\n",
    "X = X_train.drop(['RainTomorrow'], axis=1)\n",
    "\n",
    "# Update the numerical variables, we need them later\n",
    "numerical.remove(\"RainTomorrow\")\n",
    "\n",
    "del X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train/validation/test\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.2, \n",
    "    random_state=150, \n",
    "    shuffle=True,\n",
    "    stratify = y,\n",
    ")\n",
    "\n",
    "# convert to pandas dataframe\n",
    "X_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "X_valid = pd.DataFrame(X_valid, columns=X.columns)\n",
    "\n",
    "del X\n",
    "del y\n",
    "\n",
    "print(X_train.info(), \"\\n\", y_train.value_counts())\n",
    "print(X_valid.info(), \"\\n\", y_valid.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputate the missing numerical values using the median on that feature\n",
    "# Imputate the missing categorical values using the most frequent value on that feature\n",
    "\n",
    "print('\\n Missing data before imputation:\\n', X_train.isnull().sum())\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "frequent_imputer = SimpleImputer(strategy='most_frequent') \n",
    "# Your code here: train the model\n",
    "\n",
    "X_train[categorical] = frequent_imputer.transform(X_train[categorical])\n",
    "\n",
    "median_imputer = SimpleImputer(strategy='median') # imputing using constant value\n",
    "# Your code here: train the model\n",
    "\n",
    "X_train[numerical] = median_imputer.transform(X_train[numerical])\n",
    "\n",
    "print('\\n Missing data after imputation:\\n', X_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding for the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.get_dummies(X_train, columns = categorical)\n",
    "print(X_train.info())\n",
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature scaling\n",
    "We use MinMax to bring all features into [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training\n",
    "We train a logistic regression model to predict if it rains tomorrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same transformations to the validation data. \n",
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a logistic regression model\n",
    "\n",
    "logreg = LogisticRegression(\n",
    "    penalty='l2', \n",
    "    multi_class='auto', \n",
    "    max_iter=1000, \n",
    "    random_state=10,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6. What is the accuracy score on the training data (2 decimals only)? \n",
    "#### Q7. What is the accuracy score on the validation data (2 decimals only)? \n",
    "#### Q8. How many of the predictions on the test data are \"1\" (\"it will rain tomorrow\")? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the model on the test data. \n",
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
