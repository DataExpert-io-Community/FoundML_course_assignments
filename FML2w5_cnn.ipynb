{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aqSTMfy-3Su"
   },
   "source": [
    "# Deep learning with convolutional neural networks\n",
    "#### Part of the course on \"Foundations of machine learning\", Department of Mathematics and Statistics, University of Turku, Finland\n",
    "#### Lectures available on YouTube: https://youtube.com/playlist?list=PLbkSohdmxoVAZ9DEHEWHjeGK7Ei-DjKHI&si=Msu74_I0qhLrRWcu\n",
    "#### Code available on GitHub: https://github.com/ionpetre/FoundML_course_assignments\n",
    "\n",
    "#### This notebook is partially based on the following sources:\n",
    "\n",
    "> https://www.tensorflow.org/tutorials/keras/classification\n",
    "\n",
    "We demonstrate in this notebook the use of convolutional neural networks for classification. We use the tensorflow and keras as the Pyhton libraries. We also demonstrate the use of a pre-trained ResNet model.\n",
    "\n",
    "Datasets used in this notebook: MNIST, Fashion MNIST, CIFAR-10.\n",
    "\n",
    "#### This notebook uses some fairly big models. On a \"standard\" CPU, running the notebook may take a longer than usual time, on the scale of hours, perhaps up to a day or so. Using a GPU speeds this up by a factor of 10 or so. You can do small experiments on your CPU and do full trainings on a GPU platform such as Google Colaboratory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wlHx-A-L-3Sx"
   },
   "outputs": [],
   "source": [
    "# From https://www.tensorflow.org/tutorials/keras/classification:\n",
    "\n",
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2017 Fran√ßois Chollet\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a\n",
    "# copy of this software and associated documentation files (the \"Software\"),\n",
    "# to deal in the Software without restriction, including without limitation\n",
    "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
    "# and/or sell copies of the Software, and to permit persons to whom the\n",
    "# Software is furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in\n",
    "# all copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
    "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
    "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
    "# DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilLM4CcR-3Sz"
   },
   "source": [
    "#### Load the libraries and our own support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkwushhu-3S0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HXjsTNvE7iq9"
   },
   "outputs": [],
   "source": [
    "# Reset the seed of the random number generator, for reproducibility purposes\n",
    "# Reset also the Keras/tensorflow environment\n",
    "# Finally, make tensorflow deterministic.\n",
    "\n",
    "def my_reset_env(SEED):\n",
    "  from keras.utils import set_random_seed\n",
    "\n",
    "  # Make TensorFlow ops as deterministic as possible for reproducibility of results\n",
    "  # This will affect the overall performance.\n",
    "  # `enable_op_determinism()` is introduced in TensorFlow 2.9.\n",
    "  tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "  # We reset all variables implicitly instantiated by Keras/tensorflow\n",
    "  tf.keras.backend.clear_session()\n",
    "\n",
    "  # Set the seed using keras.utils.set_random_seed. This will set:\n",
    "  # 1) `numpy` seed\n",
    "  # 2) `tensorflow` random seed\n",
    "  # 3) `python` random seed\n",
    "  set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6FnU5qiv7-VI"
   },
   "outputs": [],
   "source": [
    "# This callback will stop the training when there is no improvement in the loss\n",
    "#      for three consecutive epochs.\n",
    "callback_loss_patience = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDGSibER-0RX"
   },
   "outputs": [],
   "source": [
    "my_metrics = [tf.keras.metrics.CategoricalAccuracy(),\n",
    "           #   tf.keras.metrics.TruePositives(),\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "htToAL7C_OLM"
   },
   "outputs": [],
   "source": [
    "# Plot the evolution of the loss and the accurayc throughout the epochs\n",
    "# This is useful to find over-fitting and decide on early stopping of the training.\n",
    "def plot_train_history(history_dict):\n",
    "\n",
    "  print(history_dict.keys())\n",
    "\n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "  train_loss = history_dict['loss']\n",
    "  val_loss = history_dict['val_loss']\n",
    "  train_acc = history_dict['categorical_accuracy']\n",
    "  val_acc = history_dict['val_categorical_accuracy']\n",
    "  # train_tp = np.array(history_dict['true_positives']) / X_train_std.shape[0]\n",
    "  # val_tp = np.array(history_dict['val_true_positives']) / X_valid_std.shape[0]\n",
    "  epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "\n",
    "  plt.figure(figsize=(15, 5))\n",
    "\n",
    "  plt.subplot(1,2,1)\n",
    "  plt.plot(epochs, train_loss, 'b', label='Training cat. cross-entropy')\n",
    "  plt.plot(epochs, val_loss, 'r', label='Validation cat. cross-entropy')\n",
    "  plt.title('Training and validation loss')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.legend()\n",
    "\n",
    "\n",
    "  plt.subplot(1,2,2)\n",
    "  plt.plot(epochs, train_acc, 'b', label='Training accuracy')\n",
    "  plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
    "  plt.title('Training and validation accuracy')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Categorical accuracy')\n",
    "  plt.legend()\n",
    "\n",
    "  #plt.subplot(1,3,3)\n",
    "  #plt.plot(epochs, train_tp, 'b', label='Training TP')\n",
    "  #plt.plot(epochs, val_tp, 'r', label='Validation TP')\n",
    "  #plt.title('Training and validation true positives')\n",
    "  #plt.xlabel('Epochs')\n",
    "  #plt.ylabel('True positives')\n",
    "  #plt.legend()\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFmXtDuC-3S2"
   },
   "source": [
    "## Demo a convolutional neural network classifier on the MNIST dataset\n",
    "#### We use the LeNet-5 architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ECk-YLWm-3S3",
    "outputId": "9516393f-49b3-459c-b6c6-dd759245ff10"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "\n",
    "(X_train_valid, y_train_valid), (X_test, y_test) = mnist.load_data()\n",
    "img_width = X_train_valid.shape[1]\n",
    "img_heights = X_train_valid.shape[2]\n",
    "print('The size of our training dataset: samples x width x height x channels =', X_train_valid.shape)\n",
    "\n",
    "# Reshape the data to add an extra dimension for the grayscale 'color' channel\n",
    "# Depending on the version of Keras, the color channel is either expected in fromnt, or in the end\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    X_train_valid = X_train_valid.reshape(X_train_valid.shape[0], 1, img_width, img_heights)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_width, img_heights)\n",
    "    input_shape = (1, img_width, img_heights)\n",
    "else:\n",
    "    X_train_valid = X_train_valid.reshape(X_train_valid.shape[0], img_width, img_heights, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_width, img_heights, 1)\n",
    "    input_shape = (img_width, img_heights, 1)\n",
    "\n",
    "print('The size of our training dataset: samples x width x height x channels =', X_train_valid.shape)\n",
    "print('We train on ',X_train_valid.shape[0], 'samples.')\n",
    "print('We test on ', X_test.shape[0], 'samples.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2-H8M5P-3S4"
   },
   "source": [
    "#### Data preprocessing\n",
    "The data must be preprocessed before training the network. If you inspect the first image in the training set, you will see that the pixel values fall in the range of 0 to 255:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "yE9FUe8_-3S4",
    "outputId": "ccee7aa3-6ef9-4c29-c9eb-fbc7e5039149"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(X_train_valid[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IsHNS1YG-3S5"
   },
   "outputs": [],
   "source": [
    "# Scale the data into [0,1] by dividing to 255\n",
    "\n",
    "X_train_valid_std = X_train_valid/255\n",
    "X_test_std  = X_test/255\n",
    "\n",
    "del X_train_valid\n",
    "del X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eCkQ2gyR-3S7",
    "outputId": "05c5d257-2d88-48f5-ba58-d1374680060d"
   },
   "outputs": [],
   "source": [
    "# Input size: 32x32 expected in LeNet-5, we have 28x28\n",
    "# The LeNet architecture accepts a 32x32 pixel images as input, mnist data is 28x28 pixels.\n",
    "# We simply pad the images with zeros to overcome that.\n",
    "# Another possibility is to modify the input layer size in the LeNet architecture.\n",
    "# We prefer to leave that unchaged for historical reasons.\n",
    "\n",
    "# Pad images with 0s\n",
    "X_train_valid_std = np.pad(X_train_valid_std, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "X_test_std = np.pad(X_test_std, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "\n",
    "print(\"Updated Image Shape: {}\".format(X_train_valid_std.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 717
    },
    "id": "LaCvkncc-3S8",
    "outputId": "5c2d8706-2ca5-4bdd-dab5-666562d00bc4"
   },
   "outputs": [],
   "source": [
    "# Display some images\n",
    "\n",
    "class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "plt.figure(figsize=(20,12))\n",
    "for i in range(50):\n",
    "    plt.subplot(5,10,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(X_train_valid_std[i])\n",
    "    plt.xlabel(class_names[y_train_valid[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aBRdEVKi-3S9",
    "outputId": "16dc452e-32d4-442f-83a9-92f127baf758"
   },
   "outputs": [],
   "source": [
    "# Train - validation split\n",
    "\n",
    "X_train_std, X_valid_std, y_train, y_valid = train_test_split(\n",
    "    X_train_valid_std,\n",
    "    y_train_valid,\n",
    "    test_size=0.2,\n",
    "    random_state=150,\n",
    "    stratify=y_train_valid,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Check the result of the data split\n",
    "\n",
    "print('# of training images:', X_train_std.shape[0])\n",
    "print('# of validation images:', X_valid_std.shape[0])\n",
    "\n",
    "del X_train_valid_std\n",
    "del y_train_valid\n",
    "\n",
    "# Encode the labels from numerical to categorical\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train_cat = to_categorical(y_train, num_classes=10)\n",
    "y_valid_cat = to_categorical(y_valid, num_classes=10)\n",
    "y_test_cat = to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkqzS_Kr-3S-"
   },
   "source": [
    "#### The LeNet-5 model architecture\n",
    "\n",
    "![lenet.png](https://raw.githubusercontent.com/MostafaGazar/mobile-ml/master/files/lenet.png)\n",
    "> LeNet-5 Architecture. Credit: [LeCun et al., 1998](http://yann.lecun.com/exdb/publis/psgz/lecun-98.ps.gz)\n",
    "\n",
    "#### Input\n",
    "    32x32x1 pixels image\n",
    "\n",
    "#### Architecture\n",
    "* **Convolutional #1** outputs 28x28x6\n",
    "    * **Activation** `sigmoid`\n",
    "\n",
    "* **Pooling #1** The output shape should be 14x14x6.\n",
    "\n",
    "* **Convolutional #2** outputs 10x10x16.\n",
    "    * **Activation** `sigmoid`\n",
    "\n",
    "* **Pooling #2** outputs 5x5x16.\n",
    "    * **Flatten** Flatten the output shape of the final pooling layer\n",
    "\n",
    "* **Fully Connected #1** outputs 120\n",
    "    * **Activation** `sigmoid`\n",
    "\n",
    "* **Fully Connected #2** outputs 84\n",
    "    * **Activation** `sigmoid`\n",
    "\n",
    "* **Fully Connected #3** output 10\n",
    "    * **Activation** `softmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E9XOiIRF-3S_",
    "outputId": "4dc5052e-117b-4347-8e79-18e04cced6e1"
   },
   "outputs": [],
   "source": [
    "# The model can be setup by specifying each layer:\n",
    "#          its type, its size, its activation function.\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "LeNet5model = models.Sequential([\n",
    "    layers.Conv2D(filters=6,\n",
    "                  kernel_size=(5, 5),\n",
    "                  strides=(1,1),\n",
    "                  padding='valid',\n",
    "                  activation='sigmoid',\n",
    "                  input_shape=X_train_std.shape[1:] # (32,32,1)\n",
    "                 ),\n",
    "    layers.AveragePooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(filters=16,\n",
    "                  kernel_size=(5, 5),\n",
    "                  strides=[1,1],\n",
    "                  padding='valid',\n",
    "                  activation='sigmoid'\n",
    "                 ),\n",
    "    layers.AveragePooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(units=120, activation='sigmoid'),\n",
    "    layers.Dense(units=84, activation='sigmoid'),\n",
    "    layers.Dense(units=10, activation = 'softmax')\n",
    "])\n",
    "\n",
    "LeNet5model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XiFAe4ur-3TA"
   },
   "source": [
    "Our model has 61706 parameters. Most of them (48120 + 10164 + 850) come from the dense layers. Let's see how it trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UtcqdF6D-3TA"
   },
   "outputs": [],
   "source": [
    "# The model must be compiled by specifying the numerical optimizer algorithm,\n",
    "#     the loss function, and metrics to be followed up epoch by epoch\n",
    "\n",
    "LeNet5model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=my_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eHSADgRK-3TC",
    "outputId": "0e6e84d6-9c90-4b80-bf15-772916ad375d"
   },
   "outputs": [],
   "source": [
    "# Reset the Keras/tensorflow environment, including the random number generator seed\n",
    "my_reset_env(2023)\n",
    "\n",
    "\n",
    "# Fit the model by specifying the number of epochs and the batch size\n",
    "# We also indicate the validation data so we can collect the evolution\n",
    "#      of the metrics through the epochs, both on train, as well as on validation.\n",
    "\n",
    "LeNet5_fit_history = LeNet5model.fit(X_train_std,\n",
    "                               y_train_cat,\n",
    "                               epochs=100,\n",
    "                               batch_size=500,\n",
    "                               callbacks=[callback_loss_patience],\n",
    "                               validation_data=(X_valid_std, y_valid_cat)\n",
    "                              )\n",
    "\n",
    "plot_train_history(LeNet5_fit_history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCcqaeiH-3TF"
   },
   "source": [
    "This is an amazing result, both on the training set, as well as on the validation set. Let's check the classification rerport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_KlqbS10-3TF",
    "outputId": "51ee7655-05ea-4da4-8017-ac0d59552648"
   },
   "outputs": [],
   "source": [
    "# Use the model to predict in the form of a 10-class probability distribution\n",
    "y_train_prob = LeNet5model.predict(X_train_std)\n",
    "\n",
    "# Select the most likely class\n",
    "y_train_pred=np.argmax(y_train_prob, axis=1)\n",
    "\n",
    "print(\"\\n The classification results on the train data:\")\n",
    "print(classification_report(y_train,y_train_pred))\n",
    "print(\"Confusion matrix (train data):\\n\", confusion_matrix(y_train,y_train_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The classification results for the validation data\n",
    "\n",
    "y_valid_prob = LeNet5model.predict(X_valid_std)\n",
    "y_valid_pred=np.argmax(y_valid_prob, axis=1)\n",
    "print(\"\\n The classification results on the validation data:\")\n",
    "print(classification_report(y_valid,y_valid_pred))\n",
    "print(\"Confusion matrix (validation data):\\n\", confusion_matrix(y_valid,y_valid_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The classification results for the test data\n",
    "\n",
    "y_test_prob = LeNet5model.predict(X_test_std)\n",
    "y_test_pred=np.argmax(y_test_prob, axis=1)\n",
    "print(\"\\n The classification results on the test data:\")\n",
    "print(classification_report(y_test,y_test_pred))\n",
    "print(\"Confusion matrix (test data):\\n\", confusion_matrix(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nc5KvnkU-3TG"
   },
   "outputs": [],
   "source": [
    "del X_train_std\n",
    "del X_valid_std\n",
    "del X_test_std\n",
    "del y_train\n",
    "del y_train_prob\n",
    "del y_train_pred\n",
    "del y_valid\n",
    "del y_valid_prob\n",
    "del y_valid_pred\n",
    "del y_test\n",
    "del y_test_prob\n",
    "del y_test_pred\n",
    "del LeNet5model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxD-Pvbj-3TH"
   },
   "source": [
    "## Challenge 1: train a convolutional neural network classifier on the fashion MNIST dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHq2ZcCF-3TI"
   },
   "source": [
    "#### Data: the Fashion MNIST dataset\n",
    "\n",
    "This is a dataset of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images. This dataset can be used as a drop-in replacement for MNIST.\n",
    "\n",
    "The classes are:\n",
    "\n",
    "| Label | Description   |\n",
    "|-------|---------------|\n",
    "|    0  | T-shirt/top   |\n",
    "|    1  |\tTrouser     |\n",
    "|    2  |\tPullover    |\n",
    "|    3  |\tDress       |\n",
    "|    4  |\tCoat        |\n",
    "|    5  |\tSandal      |\n",
    "|    6  |\tShirt       |\n",
    "|    7  |\tSneaker     |\n",
    "|    8  |\tBag         |\n",
    "|    9  |\tAnkle boot  |\n",
    "\n",
    "License: The copyright for Fashion-MNIST is held by Zalando SE. Fashion-MNIST is licensed under the MIT license.\n",
    "\n",
    "The data is available from the Keras datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkxNeV2bIGgR"
   },
   "source": [
    "#### Coding instructions\n",
    "\n",
    "> Use the LeNet-5 architecture, originally introduced to classify the MNIST dataset.\n",
    "\n",
    "> Loading the data is similar as for MNIST, the following code should help:\n",
    ">>from keras.datasets import fashion_mnist\n",
    ">>(X_train_valid, y_train_valid), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "> Use the 28 x 28 original images in the fashion MNIST dataset, skip the padding to size 32x32. Instead, indicate in the input layer that the input size is (28,28,1).\n",
    "\n",
    "> Scale the data by diving to 255.\n",
    "\n",
    "> Use the LeNet-5 architecture demonstrated in this notebook for the MNIST dataset, modified so that it uses 'relu' instead of the 'sigmoid' activation and the MaxPooling2D layer instead of the AveragePooling2D, with the same parameters.\n",
    "\n",
    "> Reset the tensorflow environment and the random number generator seed in exactly the same way as for the MNIST dataset (this is important for reproducibility). Do this before every re-training of the model. \n",
    "\n",
    "> Train the model for 50 epochs with the same callback function we used before. Use a batch size of 500.\n",
    "\n",
    "> Get the classification report on train, validation, and test.\n",
    "\n",
    "#### Questions\n",
    "> Q1. How many (trainable) parameters does your LeNet-5 model for Fashion MNIST have?\n",
    "\n",
    "> Q2. What is the categorical accuracy of your LeNet-5 model on the Fashion MNIST training dataset? \n",
    "\n",
    "> Q3. What is the categorical accuracy of your LeNet-5 model on the Fashion MNIST validation dataset? \n",
    "\n",
    "> Q4. What is the categorical accuracy of your LeNet-5 model on the Fashion MNIST test dataset? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2IR2skG0IZ4A"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyVYOoir-3TP"
   },
   "source": [
    "## Demo 2: train a convolutional neural network classifier on the CIFAR-10 dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MP4Jan_BPq8s"
   },
   "source": [
    "#### The CIFAR datasets\n",
    "\n",
    "The CIFAR-10 and CIFAR-100 are labeled subsets of the 80 million tiny images dataset. They were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The 10 classes are:\n",
    "\n",
    "| Label | Description   |\n",
    "|-------|---------------|\n",
    "|    0  | Airplane   |\n",
    "|    1  |\tAutomobile     |\n",
    "|    2  |\tBird    |\n",
    "|    3  |\tCat       |\n",
    "|    4  |\tDeer        |\n",
    "|    5  |\tDog      |\n",
    "|    6  |\tFrog       |\n",
    "|    7  |\tHorse     |\n",
    "|    8  |\tShip         |\n",
    "|    9  |\tTruck  |\n",
    "\n",
    "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
    "\n",
    "Webpage, including download: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "Dataset on Keras: https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsBFu4Tz9lUC"
   },
   "source": [
    "#### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GBsA-I9R-3TQ",
    "outputId": "0540596a-3044-42c2-8d71-9dab1009c404"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(X_train_valid, y_train_valid), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print('We have %2d training pictures and %2d test pictures.' % (X_train_valid.shape[0],X_test.shape[0]))\n",
    "print('Each picture is of size (%2d,%2d)' % (X_train_valid.shape[1], X_train_valid.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ACP8-paD-3TR"
   },
   "outputs": [],
   "source": [
    "# Scale the data into [0,1] by dividing to 255\n",
    "\n",
    "X_train_valid_std = X_train_valid/255\n",
    "X_test_std  = X_test/255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "PHCUafrC-3TR",
    "outputId": "d045056f-9072-41fd-baaf-3c6bd441c00f"
   },
   "outputs": [],
   "source": [
    "# Display some images\n",
    "\n",
    "class_names = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,12))\n",
    "for i in range(50):\n",
    "    plt.subplot(5,10,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(X_train_valid_std[i])\n",
    "    plt.xlabel(class_names[int(y_train_valid[i])])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cm9TpBl5-3TR",
    "outputId": "d9d6fe2e-dcde-40e2-b6a4-961ad96162e5"
   },
   "outputs": [],
   "source": [
    "# Train - validation split\n",
    "\n",
    "X_train_std, X_valid_std, y_train, y_valid = train_test_split(\n",
    "    X_train_valid_std,\n",
    "    y_train_valid,\n",
    "    test_size=0.2,\n",
    "    random_state=150,\n",
    "    stratify=y_train_valid,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Check the result of the data split\n",
    "\n",
    "print('# of training images:', X_train_std.shape[0])\n",
    "print('# of validation images:', X_valid_std.shape[0])\n",
    "print(\"Note the shape of the data (3 color channels):\", X_train_std.shape)\n",
    "\n",
    "# Encode the labels from numerical to categorical\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train_cat = to_categorical(y_train, num_classes=10)\n",
    "y_valid_cat = to_categorical(y_valid, num_classes=10)\n",
    "y_test_cat = to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88_soXYI9BhO"
   },
   "source": [
    "#### Train a LeNet-5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UQKwt-yz-3TS",
    "outputId": "00716848-1e4c-436c-f7bb-d8322e8ec325"
   },
   "outputs": [],
   "source": [
    "# Train a CNN model with an input layer of shape (32, 32, 3), accounting for the 3 color channels.\n",
    "# Try first the LeNet-5 architecture.\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "\n",
    "LeNet5model = models.Sequential([\n",
    "    layers.Conv2D(filters=6,\n",
    "                  kernel_size=(5, 5),\n",
    "                  strides=(1,1),\n",
    "                  padding='valid',\n",
    "                  activation='relu',\n",
    "                  input_shape=X_train_std.shape[1:]  #(32,32,3)\n",
    "                 ),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(filters=16,\n",
    "                  kernel_size=(5, 5),\n",
    "                  strides=[1,1],\n",
    "                  padding='valid',\n",
    "                  activation='relu'\n",
    "                 ),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(units=120, activation='relu'),\n",
    "    layers.Dense(units=84, activation='relu'),\n",
    "    layers.Dense(units=10, activation = 'softmax')\n",
    "])\n",
    "\n",
    "LeNet5model.summary()\n",
    "\n",
    "LeNet5model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=my_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vba6J6O-MjDc",
    "outputId": "32f5e774-696f-4ed2-cd4a-9376c9f801df"
   },
   "outputs": [],
   "source": [
    "# Reset the Keras/tensorflow environment, including the random number generator seed\n",
    "my_reset_env(2023)\n",
    "\n",
    "\n",
    "# Fit the model by specifying the number of epochs and the batch size\n",
    "# We also indicate the validation data so we can collect the evolution\n",
    "#      of the metrics through the epochs, both on train, as well as on validation.\n",
    "\n",
    "LeNet5_fit_history = LeNet5model.fit(X_train_std,\n",
    "                               y_train_cat,\n",
    "                               epochs=100,\n",
    "                               batch_size=500,\n",
    "                               callbacks=[callback_loss_patience],\n",
    "                               validation_data=(X_valid_std, y_valid_cat)\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "iaBtTl1a-3TT",
    "outputId": "dc1d201d-fbf4-48b3-d4ab-0208ad76224d"
   },
   "outputs": [],
   "source": [
    "plot_train_history(LeNet5_fit_history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OChy9jGLDUcc"
   },
   "source": [
    "The model overfits from epoch or so 40 onwards. Its accuracy is only about 60% on the validation data. Let's try a bigger and deeper model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OsrVDl3DDj5x"
   },
   "outputs": [],
   "source": [
    "del LeNet5model\n",
    "del LeNet5_fit_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOoul4fN9LIg"
   },
   "source": [
    "## Challenge 2: classify CIFAR-10 with about 80% accuracy on the validation set.\n",
    "\n",
    "#### Coding instructions:\n",
    "> Layer 1: a convolutional layer with 32 filters, kernel size (3,3), stride 1, padding 'same', nad activation 'relu'\n",
    "\n",
    "> Layer 2: a batch normalization layer that nornalizess the activation output of the first later (declare it using \"layers.BatchNormalization()\")\n",
    "\n",
    "> Layer 3: the same as layer 1\n",
    "\n",
    "> Layer 4: the same as layer 2\n",
    "\n",
    "> Layer 5: a max pooling layer of size (2,2) with strides 2\n",
    "\n",
    "> Layer 6: a dropout layer with rate 0.25 (declare it using \"layers.Dropout(0.25)\"). Its role is one of regularization, to help avoiding the overfitting of the mode. During each iteration of training, 25% of the activations from the previous layer, chosen randomly, are nulified. \n",
    "\n",
    "> Layers 7-12: similar to layers 1-6, except that we use now 64 filters in the convolutional layers.\n",
    "\n",
    "> Layer 13: flatten the input\n",
    "\n",
    "> Layer 14: a dense layer with 64 neurons and 'relu' activation\n",
    "\n",
    "> Layer 15: dropout with rate 0.25 (which is it to say 25%)\n",
    "\n",
    "> Layer 16: a dense layer with 10 neurons and 'softmax' activation. \n",
    "\n",
    "> Reset the tensorflow environment and the random number generator seed in exactly the same way as for the MNIST dataset (this is important for reproducibility). Do this before every re-training of the model. \n",
    "\n",
    "> Train the model for 100 epochs with the same callback function we used before. Use a batch size of 500.\n",
    "\n",
    "> Get the classification report on train, validation, and test.\n",
    "\n",
    "#### Questions\n",
    "> Q5. How many trainable parameters does your CNN model for CIFAR-10 have? \n",
    "\n",
    "> Q6. What is its categorical accuracy of your CNN model for CIFAR-10 on the training set? \n",
    "\n",
    "> Q7. What is its categorical accuracy of your CNN model for CIFAR-10 on the validation set? \n",
    "\n",
    "> Q8. Do you consider your CNN model for CIFAR-10 to be overfit (in other words, is the loss on the validation set clearly increasing through the later stage of training)? \n",
    "\n",
    "> Q9. What is its categorical accuracy of your CNN model for CIFAR-10 on the test set? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ilLM4CcR-3Sz",
    "xFmXtDuC-3S2",
    "gxD-Pvbj-3TH",
    "AsBFu4Tz9lUC",
    "88_soXYI9BhO"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
