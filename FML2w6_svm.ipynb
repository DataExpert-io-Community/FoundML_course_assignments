{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support vector machine classifier and the kernel trick\n",
    "#### Part of the course on \"Foundations of machine learning\", Department of Mathematics and Statistics, University of Turku, Finland\n",
    "#### Lectures available on YouTube: https://youtube.com/playlist?list=PLbkSohdmxoVAZ9DEHEWHjeGK7Ei-DjKHI&si=Msu74_I0qhLrRWcu\n",
    "#### Code available on GitHub: https://github.com/ionpetre/FoundML_course_assignments\n",
    "\n",
    "Support Vector Machines (SVMs) are powerful supervised learning models. They work by finding the optimal hyperplane that best separates data points into distinct classes while maximizing the margin, the distance between the hyperplane and the closest points (support vectors). They use a kernel trick to transform data into a higher-dimensional space, enabling the creation of nonlinear decision boundaries, effectively handling complex relationships in the data. \n",
    "\n",
    "Datasets used in this notebook: Iris, two spiral dataset, checker board dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-09T17:34:11.060373Z",
     "iopub.status.busy": "2021-09-09T17:34:11.059927Z",
     "iopub.status.idle": "2021-09-09T17:34:12.616363Z",
     "shell.execute_reply": "2021-09-09T17:34:12.615487Z",
     "shell.execute_reply.started": "2021-09-09T17:34:11.06034Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the seed of the random number generator, for reproducibility purposes\n",
    "\n",
    "import os\n",
    "\n",
    "def reset_seed(SEED = 0):\n",
    "    \"\"\"Reset the seed for every random library in use (System, numpy)\"\"\"\n",
    "\n",
    "    os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "\n",
    "reset_seed(2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data: the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Iris dataset from the sklearn library. \n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X, y = load_iris(return_X_y=True, as_frame=True)\n",
    "\n",
    "# Join X and y for a moment, just for visualisation purposes, to see that the classes are separable\n",
    "Xy = pd.concat([X,y], axis=1)\n",
    "display(Xy)\n",
    "sns.pairplot(data = Xy, hue = \"target\", palette='tab10')\n",
    "del Xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X.values, \n",
    "    y, \n",
    "    test_size=0.20, \n",
    "    shuffle=True,\n",
    "    random_state=100,\n",
    "    stratify=y,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "#my_scaler = StandardScaler()\n",
    "my_scaler = MinMaxScaler()\n",
    "my_scaler = my_scaler.fit(X_train)\n",
    "X_train = my_scaler.transform(X_train)\n",
    "X_test = my_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train several support vector classifiers to demonstrate the role of the kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We evaluate the models using a 5-fold cross-validation strategy to get their performance\n",
    "# Note: this is not the same as training a single model and getting its metrics\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def cv_evaluation(estimators, X_train, y_train):\n",
    "\n",
    "    scores = pd.DataFrame(columns=['Estimator', 'CV Scores mean', 'CV Scores Variance'])\n",
    "\n",
    "    for i in range(len(estimators)):\n",
    "        est = estimators[i]\n",
    "        est_name = estimator_type[i]\n",
    "        cv_scores = cross_val_score(est, X_train, y_train, cv=5, n_jobs=-1)\n",
    "        scores.loc[i] = [est_name, cv_scores.mean(), cv_scores.std()**2]\n",
    "    \n",
    "    scores.sort_values(by='CV Scores mean', ascending=False, inplace=True)\n",
    "    print(scores)\n",
    "\n",
    "    plt.figure(figsize=(5, 2))\n",
    "    sns.barplot(x=scores['CV Scores mean'], y=scores['Estimator'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We plot the decision boundaries of our models\n",
    "\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "def plot_decision_boundary(estimators, X_train, y_train):\n",
    "    \n",
    "    # center the plot onto the data spread\n",
    "    x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "    y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "\n",
    "    # Train the models\n",
    "\n",
    "    for i in range(len(estimators)):    \n",
    "        estimators[i].fit(X_train, y_train)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10,10))\n",
    "\n",
    "    for i in range(len(estimators)):\n",
    "        axes[i//2, i%2].set(title=estimator_type[i])\n",
    "        axes[i//2, i%2].set_xlim(x_min, x_max)\n",
    "        axes[i//2, i%2].set_ylim(y_min, y_max)\n",
    "        disp = DecisionBoundaryDisplay.from_estimator(\n",
    "            estimators[i], X_train, response_method=\"predict\",\n",
    "            xlabel=X.columns[0], ylabel=X.columns[1],\n",
    "            alpha=0.5,\n",
    "            ax=axes[i//2, i%2],\n",
    "            plot_method='contourf'\n",
    "        )\n",
    "        disp.ax_.scatter(X_train[:,0], X_train[:,1], c=y_train, edgecolor=\"k\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will test several levels of regularization, indicated through parameter C\n",
    "\n",
    "def my_estimators(C):\n",
    "\n",
    "    estimators = [\n",
    "        SVC(C=C,            # Regularization parameter. The strength of the regularization is inversely proportional to C. \n",
    "            kernel='linear',  # ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’\n",
    "            decision_function_shape='ovr', \n",
    "            random_state=150),\n",
    "        #\n",
    "        SVC(C=C,            # Regularization parameter. The strength of the regularization is inversely proportional to C. \n",
    "            kernel='poly',     # ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’\n",
    "            degree=3,         # Degree of the polynomial kernel function (‘poly’).\n",
    "            gamma='scale',    # Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. ‘scale’, ‘auto’, or float. \n",
    "            decision_function_shape='ovr', \n",
    "            random_state=150),\n",
    "    #\n",
    "        SVC(C=C,            # Regularization parameter. The strength of the regularization is inversely proportional to C. \n",
    "            kernel='sigmoid',     # ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’\n",
    "            gamma='auto',    # Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. ‘scale’, ‘auto’, or float. \n",
    "            decision_function_shape='ovr', \n",
    "            random_state=150),\n",
    "    #\n",
    "        SVC(C=1.0,            # Regularization parameter. The strength of the regularization is inversely proportional to C. \n",
    "            kernel='rbf',     # ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’\n",
    "            gamma='scale',    # Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. ‘scale’, ‘auto’, or float. \n",
    "            decision_function_shape='ovr', \n",
    "            random_state=150),\n",
    "    ]\n",
    "\n",
    "    return estimators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_type = ['linear', 'poly', 'sigmoid', 'rbf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the visualisation of the decision boundaries we only use the first two columns\n",
    "X_train_2 = X_train[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will test several levels of regularization. \n",
    "# We start with C=1, our base level of regularization.\n",
    "\n",
    "reset_seed(2023)\n",
    "estimators = my_estimators(1.0)\n",
    "cv_evaluation(estimators, X_train, y_train)\n",
    "plot_decision_boundary(estimators, X_train_2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test C=10\n",
    "# This gives less regularization, we expect the models (the decision boundaries) to become more complex. \n",
    "\n",
    "reset_seed(2023)\n",
    "estimators = my_estimators(10.0)\n",
    "cv_evaluation(estimators, X_train, y_train)\n",
    "plot_decision_boundary(estimators, X_train_2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C=0.1\n",
    "# More regularization. We expect the models and the decision boundaris to be simpler.\n",
    "\n",
    "reset_seed(2023)\n",
    "estimators = my_estimators(0.1)\n",
    "cv_evaluation(estimators, X_train, y_train)\n",
    "plot_decision_boundary(estimators, X_train_2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X\n",
    "del y\n",
    "del X_train\n",
    "del X_train_2\n",
    "del y_train\n",
    "del X_test\n",
    "del y_test\n",
    "del estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1: train support vector classifiers for the two spirals dataset\n",
    "We train an SVC for a synthetic dataset that is designed to be \"difficult\": it is a 2-class dataset consisting of points spiraling around each other. Obviously, the dataset is not linearly separable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the 2-spirals dataset X\n",
    "\n",
    "import math\n",
    "\n",
    "def spiral_xy(i, spiral_num):\n",
    "    \"\"\"\n",
    "    Create the data for a spiral.\n",
    "\n",
    "    Arguments:\n",
    "        i runs from 0 to 96\n",
    "        spiral_num is 1 or -1\n",
    "    \"\"\"\n",
    "    φ = i/16 * math.pi\n",
    "    r = 6.5 * ((104 - i)/104)\n",
    "    x = (r * math.cos(φ) * spiral_num)/13 + 0.5\n",
    "    y = (r * math.sin(φ) * spiral_num)/13 + 0.5\n",
    "    return (x, y)\n",
    "\n",
    "def spiral(spiral_num):\n",
    "    return [spiral_xy(i, spiral_num) for i in range(97)]\n",
    "\n",
    "a = pd.DataFrame(np.array(spiral(1)), columns=['x', 'y'])\n",
    "a['label']=1\n",
    "b = pd.DataFrame(np.array(spiral(-1)), columns=['x', 'y'])\n",
    "b['label']=-1\n",
    "X = pd.concat([a,b], axis=0)\n",
    "\n",
    "X.plot.scatter(x='x', y='y', c='label', colormap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=X[['label']]\n",
    "X.drop(['label'], axis=1, inplace=True)\n",
    "X_train = X.to_numpy()\n",
    "y_train = y.to_numpy()\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "my_scaler = StandardScaler()\n",
    "my_scaler = my_scaler.fit(X)\n",
    "X_train = my_scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge: learn the 2-spiral dataset using a support vector classifier\n",
    ">Train a support vector classifier to learn perfectly the 2-spirals dataset. Try the four kernels dmeonstrated in this notebook. \n",
    "\n",
    ">You can keep C=1 and check different values for parameter gamma for the polynomial, the sigmoid and the radial basis kernels. Use degree 10 for the polynomial kernel.\n",
    "\n",
    ">In each case, train the model and display its decision boundary support. Get its predictions on X_train and get the classification report to obtain the accuracy.\n",
    "\n",
    "> Use reset_seed(2023) before each call to model fitting, as done in the demo part of this notebook.\n",
    "\n",
    "> Q1. What is the accuracy of the linear kernel? \n",
    "\n",
    "> Q2-4. For gamma='scale' what is the accuracy for the poly/sigmoid/rbf SVC? \n",
    "\n",
    "> Q5-6. For gamma=150 what is the accuracy for the sigmoid/rbf SVC? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will test several levels of regularization, indicated through parameters C and gamma\n",
    "\n",
    "def my_estimators(C, gamma):\n",
    "\n",
    "    estimators = [\n",
    "        SVC(C=C,            # Regularization parameter. The strength of the regularization is inversely proportional to C. \n",
    "            kernel='linear',  # ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’\n",
    "            decision_function_shape='ovr', \n",
    "            random_state=150),\n",
    "        #\n",
    "        SVC(C=C,            # Regularization parameter. The strength of the regularization is inversely proportional to C. \n",
    "            kernel='poly',     # ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’\n",
    "            degree=10,         # Degree of the polynomial kernel function (‘poly’).\n",
    "            gamma='scale',    # Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. ‘scale’, ‘auto’, or float. \n",
    "            decision_function_shape='ovr', \n",
    "            random_state=150),\n",
    "    #\n",
    "        SVC(C=C,            # Regularization parameter. The strength of the regularization is inversely proportional to C. \n",
    "            kernel='sigmoid',     # ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’\n",
    "            gamma=gamma,    # Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. ‘scale’, ‘auto’, or float. \n",
    "            decision_function_shape='ovr', \n",
    "            random_state=150),\n",
    "    #\n",
    "        SVC(C=1.0,            # Regularization parameter. The strength of the regularization is inversely proportional to C. \n",
    "            kernel='rbf',     # ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’\n",
    "            gamma=gamma,    # Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. ‘scale’, ‘auto’, or float. \n",
    "            decision_function_shape='ovr', \n",
    "            random_state=150),\n",
    "    ]\n",
    "\n",
    "    return estimators \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We evaluate our models\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, RocCurveDisplay\n",
    "\n",
    "def evaluate(estimators, X_train, y_train):\n",
    "    \n",
    "   # fig, axes = plt.subplots(2, 2, figsize=(10,10))\n",
    "\n",
    "    for i in range(len(estimators)):    \n",
    "        estimators[i].fit(X_train, y_train.ravel())\n",
    "        y_train_pred=estimators[i].predict(X_train)        \n",
    "        print(\"\\n The classification results on the train data (\",estimator_type[i],\"):\")\n",
    "        print(classification_report(y_train,y_train_pred))\n",
    "        print(\"Confusion matrix on the train data(\",estimator_type[i],\"):\\n\", confusion_matrix(y_train,y_train_pred))\n",
    "\n",
    "        RocCurveDisplay.from_estimator(estimators[i], X_train, y_train)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2:  train support vector classifiers for the checkerboard dataset. \n",
    ">Train an SVC for another synthetic dataset that is designed to be \"difficult\": the checkerboard dataset. Obviously, the dataset is not linearly separable. Try the four kernels dmeonstrated in this notebook. \n",
    "\n",
    ">You can keep C=1 and check different values for parameter gamma for the polynomial, the sigmoid and the radial basis kernels. Use degree 10 for the polynomial kernel.\n",
    "\n",
    ">In each case, train the model and display its decision boundary support. Get its predictions on X_train and get the classification report to obtain the accuracy.\n",
    "\n",
    "> Use reset_seed(2023) before each call to model fitting, as done in the demo part of this notebook.\n",
    "\n",
    "> Q7. What is the accuracy of the linear kernel? \n",
    "\n",
    "> Q8-10. For gamma='scale' what is the accuracy for the poly/sigmoid/rbf SVC?\n",
    "\n",
    "> Q11-12. For gamma=50 what is the accuracy for the sigmoid/rbf SVC? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We generate a checkerboard 4 x 4, where each cell (i,j) consists of 100 points with \n",
    "# coordinates (x,y), all with the same label. \n",
    "# The labels alternate between 0 and 1 from cell to cell, horizontally and vertically. \n",
    "# Each cell has size width x height\n",
    "\n",
    "width = 2\n",
    "height = 2 \n",
    "\n",
    "from numpy.random import rand\n",
    "\n",
    "checkerboard = pd.DataFrame(columns=['x', 'y', 'color'])\n",
    "checkerboard = pd.DataFrame({'x': pd.Series(dtype='int'), \n",
    "                             'y': pd.Series(dtype='int'), \n",
    "                             'color': pd.Series(dtype='int')\n",
    "                            })\n",
    "for i in range(4):\n",
    "    for j in range (4):\n",
    "        # Generate the points in cell (i,j): array representing the (x,y) coordinates of each point\n",
    "        ij = rand(100,2)\n",
    "        ij[:,0] = i*width + ij[:,0]*width\n",
    "        ij[:,1] = j*height + ij[:,1]*height\n",
    "        ij_df=pd.DataFrame(ij, columns=['x', 'y'])\n",
    "        ij_df['color']=(i+j)%2\n",
    "        checkerboard = pd.concat([checkerboard, ij_df], axis=0)\n",
    "        \n",
    "\n",
    "checkerboard.plot.scatter(x='x', y='y', c='color', colormap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=checkerboard[['color']]\n",
    "X=checkerboard.drop(['color'], axis=1, inplace=False)\n",
    "X_train = X.to_numpy()\n",
    "y_train = y.to_numpy()\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "my_scaler = StandardScaler()\n",
    "my_scaler = my_scaler.fit(X)\n",
    "X_train = my_scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will test several levels of regularization, indicated through parameter C\n",
    "\n",
    "def my_estimators(C, gamma):\n",
    "\n",
    "    estimators = [\n",
    "        SVC(C=C,            # Regularization parameter. The strength of the regularization is inversely proportional to C. \n",
    "            kernel='linear',  # ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’\n",
    "            decision_function_shape='ovr', \n",
    "            random_state=150),\n",
    "        #\n",
    "        SVC(C=C,            # Regularization parameter. The strength of the regularization is inversely proportional to C. \n",
    "            kernel='poly',     # ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’\n",
    "            degree=10,         # Degree of the polynomial kernel function (‘poly’).\n",
    "            gamma='scale',    # Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. ‘scale’, ‘auto’, or float. \n",
    "            decision_function_shape='ovr', \n",
    "            random_state=150),\n",
    "    #\n",
    "        SVC(C=C,            # Regularization parameter. The strength of the regularization is inversely proportional to C. \n",
    "            kernel='sigmoid',     # ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’\n",
    "            gamma=gamma,    # Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. ‘scale’, ‘auto’, or float. \n",
    "            decision_function_shape='ovr', \n",
    "            random_state=150),\n",
    "    #\n",
    "        SVC(C=1.0,            # Regularization parameter. The strength of the regularization is inversely proportional to C. \n",
    "            kernel='rbf',     # ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’\n",
    "            gamma=gamma,    # Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. ‘scale’, ‘auto’, or float. \n",
    "            decision_function_shape='ovr', \n",
    "            random_state=150),\n",
    "    ]\n",
    "\n",
    "    return estimators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
