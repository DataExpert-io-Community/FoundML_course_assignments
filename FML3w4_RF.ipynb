{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "colab": {
      "name": "Student version of RF assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJEzefsa7K8o"
      },
      "source": [
        "# Random Forest - Tutorial\n",
        "\n",
        "In this project we will train a Random Forest model to classify data from the breast cancer dataset available in the sklearn.datasets library. The first part of this notebook is a tutorial section which shows how to access the data, how to examine the data and how to create and train a Random Forest model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-10-12T17:00:50.082316Z",
          "start_time": "2021-10-12T17:00:49.976968Z"
        },
        "id": "-5roXtUleU0o"
      },
      "source": [
        "# Necessary libraries for data exploration\n",
        "import numpy as np # matrix operations\n",
        "import pandas as pd # Dataframes\n",
        "\n",
        "# Datasets are available through this\n",
        "import sklearn.datasets as datasets\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkqfMiVg5wAb"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OClEInqq5ylu"
      },
      "source": [
        "The iris-dataset provided by sklearn is accessed by the following code (there are methods defined for all available datasets). The dataset is in a dictionary format. The information inside the dictionary is accessible via indexing with the \"keys\" of the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-10-12T16:20:29.807778Z",
          "start_time": "2021-10-12T16:20:29.783748Z"
        },
        "id": "wMVFqBCoeU0w"
      },
      "source": [
        "tutorial_data = datasets.load_iris()\n",
        "print(\"Availbale information in data:\")\n",
        "print(tutorial_data.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB7djxVZ6wyy"
      },
      "source": [
        "For example, the target values (also known as the labels in labeled data) are accessed by using the \"target\" key and the actual data is accessed via the \"data\" key."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax6cRJDYeU0x"
      },
      "source": [
        "# Accessing different parts of the data\n",
        "print(\"First datapoint in the dataset:\", tutorial_data[\"data\"][0])\n",
        "print(\"Target value of the first instance: \", tutorial_data[\"target\"][0],  \"= (\" + tutorial_data[\"target_names\"][tutorial_data[\"target\"][0]] + \")\")\n",
        "print(\"The names of the targets:\", tutorial_data[\"target_names\"])\n",
        "print(\"The features available for every datapoint:\", tutorial_data[\"feature_names\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUtTC7009ba_"
      },
      "source": [
        "We now split the data into three separate sets: The training set, validation set and test set. The training set includes the data and the respective targets which the model will be trained on. The validation set is used to evaluate the model after training. Once the model performs well on the validation set, the model is then evaluated on the test set. This 2-step evaluation is done to avoid bias towards the training set and validation set, since it may work well on both sets but might still fail on other unseen data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-10-12T16:20:11.168287Z",
          "start_time": "2021-10-12T16:20:09.538Z"
        },
        "id": "HtCaEbYoeU0y"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "# Shuffle the dataset\n",
        "tutorial_data[\"data\"], tutorial_data[\"target\"] = shuffle(tutorial_data[\"data\"], tutorial_data[\"target\"])\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Separation into training and test sets\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(tutorial_data[\"data\"], tutorial_data[\"target\"], test_size=0.3)\n",
        "\n",
        "# Separation of test set into test and validation\n",
        "test_data, validation_data, test_labels, validation_labels = train_test_split(test_data, test_labels, test_size = 0.5)\n",
        "\n",
        "# Show the amount of target categories in each dataset\n",
        "print(np.bincount(train_labels), np.bincount(validation_labels), np.bincount(test_labels))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIUnEBWChp47"
      },
      "source": [
        "## Data exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYNrqUoYZF-r"
      },
      "source": [
        "For further analysis we can display the data in a number of ways. Here we simply plot the data instances as lines, the points between the lines show the value of the respective feature. Take the example below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "cLOKHtrIeU0z"
      },
      "source": [
        "# Change size of plot\n",
        "plt.figure(figsize=(13, 7))\n",
        "\n",
        "# Plot all setosa instances as blue lines\n",
        "plt.plot(train_data[train_labels == 0].T, color = \"blue\")\n",
        "\n",
        "# Plot all versicolor instances as red lines\n",
        "plt.plot(train_data[train_labels == 1].T, color = \"red\")\n",
        "\n",
        "# Plot all virginica instances as green lines\n",
        "plt.plot(train_data[train_labels == 2].T, color = \"green\")\n",
        "\n",
        "# Set the labels on the x-axis equal to the feature names\n",
        "plt.xticks(np.arange(len(tutorial_data[\"feature_names\"])),tutorial_data[\"feature_names\"])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqpjKCJhZh_x"
      },
      "source": [
        "We can see that the instances in the training set follow select patterns depending on which target class the instances belong to. In the figure above, the blue lines (instances belonging to the setosa class) show that the petal length and petal width vary significantly from the other classes. The red (versicolor) and green (virginica) instances also vary at those points (red instances are mainly below green instances for the features petal length and petal width). Based on this, we can see that petal length and width are good features for dividing the data into the different target categories.\n",
        "\n",
        "Note that these kinds of patterns aren't always so trivial in other scenarios where the data may be noisy for example.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "557QE6cbbcpl"
      },
      "source": [
        "## Random Forest\n",
        "\n",
        "Following the exploration stage we can move on to creating an instance of a random forest model. A random forest model can take a number of parameters to optimize the training of the model. In this project we mainly consider:\n",
        "- The numer of estimators in the model (how many decision trees the forest consists of)\n",
        "- The function for determining the quality of a split (sklearn supports \"gini\" and \"entropy\")\n",
        "- The depth of the estimators (how big each decision tree is allowed to get)\n",
        "\n",
        "\n",
        "For more information about the available functionality of random forest models in sklearn, visit: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-10-12T16:20:03.967002Z",
          "start_time": "2021-10-12T16:20:03.820883Z"
        },
        "id": "WP7QXUsxeU00"
      },
      "source": [
        "# Get the algorithm from sklearn\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Create a model instance\n",
        "rf = RandomForestClassifier(n_estimators = 100, criterion=\"gini\", max_depth = None, random_state= 0)\n",
        "\n",
        "# Train the model on training data\n",
        "rf.fit(train_data, train_labels)\n",
        "\n",
        "# Get model prediciotns on training data\n",
        "predictions = rf.predict(train_data)\n",
        "\n",
        "# Calculate accuracy on training\n",
        "train_acc = sum(predictions ==  train_labels)/len(train_labels)\n",
        "print(\"Final training accuracy:\", train_acc)\n",
        "\n",
        "# Get accuracy on validation set\n",
        "validation_accuracy = sum(rf.predict(validation_data) ==  validation_labels)/len(validation_labels)\n",
        "print(\"Validation accuracy:\", validation_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BErvUrOGdmN7"
      },
      "source": [
        "Provided the validation accuracy is good, we can then test the model on the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vo33pYFoeU01"
      },
      "source": [
        "# Accuracy of test set once training and validation are \"good enough\"\n",
        "test_accuracy = sum(rf.predict(test_data) ==  test_labels)/len(test_labels)\n",
        "print(\"test accuracy:\", test_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPO2pgyReZqO"
      },
      "source": [
        "Finally, one of the exciting things about random forest models is the ability to calculate the most \"predictive\" features of the dataset i.e. which features are most valuable when predicting the target value of a given data instance. This is done automatically during training and the feature importance is stored within the model itself. You can access this information via the following attribute "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6OBzgDPeU04"
      },
      "source": [
        "rf.feature_importances_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EaMlSawf2JP"
      },
      "source": [
        "The array gives percentages of how important the respective features are in making predictions. The index of the array corresponds with the order of the features in the data. Based on the scores, we can see that the features petal length and width have higher importance in categorization than sepal length and sepal width."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUrxvDRhhdDZ"
      },
      "source": [
        "#Random Forest - Project\n",
        "\n",
        "your task is to perform a similar analysis of a dataset consisting of breast cancer tumours. The tasks are listed as follows:\n",
        "- Find out what the features and the target values are\n",
        "- Sort the data into three separate datasets\n",
        "- Analyse the data by visualization, can you find any patterns in the data which can be used to determine target value?\n",
        "- Create a random forest model for categorizing the data instances into the target categories and calculating feature importance.\n",
        "\n",
        "Feel free to copy and test the code given in the tutorial above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRS09ZdCdgi6"
      },
      "source": [
        "import random\n",
        "random.seed(2021)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e2paGS4jMdH"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJmv3uHQhh5l"
      },
      "source": [
        "Cancer_data = datasets.load_breast_cancer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RP_41dTvimp"
      },
      "source": [
        "### Data\n",
        "\n",
        "From the documentation: https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset\n",
        "\n",
        "\n",
        "\"This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets. https://goo.gl/U2Uwz2\n",
        "\n",
        "Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\n",
        "\n",
        "Separating plane described above was obtained using Multisurface Method-Tree (MSM-T) [K. P. Bennett, “Decision Tree Construction Via Linear Programming.” Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society, pp. 97-101, 1992], a classification method which uses linear programming to construct a decision tree. Relevant features were selected using an exhaustive search in the space of 1-4 features and 1-3 separating planes.\n",
        "\n",
        "The actual linear program used to obtain the separating plane in the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: “Robust Linear Programming Discrimination of Two Linearly Inseparable Sets”, Optimization Methods and Software 1, 1992, 23-34].\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JQY3tPPjV3o"
      },
      "source": [
        "## Examine data structure\n",
        "\n",
        "Try to find out what the dataset consists of by printing the feature names and the target names.\n",
        "\n",
        "Q1. How many instances are available in the complete dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExXyoY5xjamu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAqk5KC7jZXU"
      },
      "source": [
        "## Sort data into different sets\n",
        "\n",
        "Create a training, validation and test set from the dataset, you can choose how big you want each set to be but make sure no instance appears in more than one set! (Remember to shuffle the data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqMR4GdPjYfw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_3YITxljgP4"
      },
      "source": [
        "## Analyze the data\n",
        "\n",
        "Use matplotlib (or any other means) to visualise the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7_8zbQLjgpP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ireUuQjSkTZ1"
      },
      "source": [
        "## Random forest model\n",
        "\n",
        "Create your model instane here. Calculate the accuracy of the model on the validation and the test set.\n",
        "\n",
        "Train the model in different setups using the gini criterion and the entropy criterion. Use 20 decision trees in yuor model, of maximum depth 3. \n",
        "\n",
        "Q2. Which feature is the most important one?\n",
        "\n",
        "Q3. What class does the model predict for an input consisting only of zeros?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wY2s4wpkVsN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}